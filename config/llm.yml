dataset:
  name: "llm_dataset"
  encoder: "gpt2"
  max_length: 32
  stride: 32

dataloader:
  name: "llm_dataloader"
  batch_size: 8
  shuffle: False
  drop_last: True
  num_workers: 0

train_dataloader:
  name: "llm_train"
  batch_size: 8
  shuffle: True
  drop_last: False
  num_workers: 0

embeddings:
  name: "llm_embeddings"
  vocabulary_size: 50257
  output_dim: 192
  context_length: 128

model:
  name: "llm_model"
  vocab_size: 50257
  context_length: 128
  embedding_dim: 192
  heads: 6
  layers: 6
  droprate: 0.2
  qkv_bias: False

training:
  name: "llm_trainer"
  epochs: 10
  eval_freq: 50
  eval_iter: 50
  learning_rate: 0.0004
  weight_decay: 0.1
  
