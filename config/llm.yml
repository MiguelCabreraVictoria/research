dataset:
  name: "llm_dataset"
  encoder: "gpt2"
  max_length: 256
  stride: 256

dataloader:
  name: "llm_dataloader"
  batch_size: 2
  shuffle: False
  drop_last: False
  num_workers: 0

train_dataloader:
  name: "llm_train"
  batch_size: 2
  shuffle: True
  drop_last: True
  num_workers: 0

embeddings:
  name: "llm_embeddings"
  vocabulary_size: 50257
  output_dim: 768
  context_length: 768

model:
  name: "llm_model"
  vocab_size: 50257
  context_length: 256
  embedding_dim: 768
  heads: 12
  layers: 12
  droprate: 0.1
  qkv_bias: False

training:
  name: "llm_trainer"
  epochs: 30
  eval_freq: 5
  eval_iter: 1
  learning_rate: 0.0005
  weight_decay: 0.01
  
